<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Dave J Kane</title>
    <link>https://davejkane.github.io/categories/programming/index.xml</link>
    <description>Recent content on Dave J Kane</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-uk</language>
    <atom:link href="https://davejkane.github.io/categories/programming/index.xml" rel="self" type="application/rss+xml" />
    
    <item>
      <title>JSON to SQLite to csv with Python</title>
      <link>https://davejkane.github.io/post/json-sqlite-csv/</link>
      <pubDate>Sun, 20 Nov 2016 00:00:00 +0000</pubDate>
      
      <guid>https://davejkane.github.io/post/json-sqlite-csv/</guid>
      <description>

&lt;h1 id=&#34;json-to-sqlite-to-csv-with-python&#34;&gt;JSON to SQLite to csv with python.&lt;/h1&gt;

&lt;p&gt;So if you read my last post about webscraping and saving a whole bunch of JSON files, then there&amp;rsquo;s a small chance that you might have just that - a whole bunch of similar JSON files. In this post I&amp;rsquo;ll run you through my process to take JSON data, saved across multiple files and transform that data into SQLite tables. In my case it&amp;rsquo;s a whole bunch of football stats saved in JSON format that I want to enter into a highly normalised SQL database. I&amp;rsquo;ll then run through how to get the SQLite data into csv format. We&amp;rsquo;ll use the command line SQLite tool for this.&lt;/p&gt;

&lt;h2 id=&#34;json-to-objects&#34;&gt;JSON to objects&lt;/h2&gt;

&lt;p&gt;Absolutely the first thing you want to do is to try to describe the domain, to make a domain model. And then to think about how these models fit together. Think of it as Domain Driven Design (DDD) light. This isn&amp;rsquo;t so much about decoupling business logic from service layers as would the case in proper DDD. At the end of the day this is scripting, we&amp;rsquo;re not building software here.&lt;/p&gt;

&lt;p&gt;The focus will be on describing the domain and using composition to handle the creation of objects. I want to make a highly normalised database. For the kind of thing where you are going to be writing very few queries, but where the queries are complex, it makes sense to normalise our data. That means that goals will be stored separately from matches and to get the goals from any given match, will require a table join. It also means to get all the goals for any given player, we won&amp;rsquo;t need to join through a Match table or anything like that. So for example, where many football databases save information about Red Cards, or Yellow Cards right in the match table, I like to have those in their own tables with a Foreign Key to the match table and to the player table and to the team table.&lt;/p&gt;

&lt;p&gt;For each python class I recommend a minimum of three methods&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;class myClass:
    def __init__():
        #Initialise the object, obvious choice.
    def __str__():
        #Handle printing behaviour. Really useful to make sure the objects have been created properly. 
    def Insert():
        #Handle inserting the object data into the SQLite database.
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The Insert() method is really going to clean up the code in your script, especially when you have a list of objects&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;for myObject in myObjects:
    myObject.Insert()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Simple. So basically, I recommend breaking the objects down into as many separate things as is practical. We don&amp;rsquo;t want to store xml strings or anything in our databases because we&amp;rsquo;re trying to cram too much information into our tables. In my database I settled for the following tables, and thus objects.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Season - Contains the name of the competition and the year.&lt;/li&gt;
&lt;li&gt;Referee&lt;/li&gt;
&lt;li&gt;Team&lt;/li&gt;
&lt;li&gt;Player&lt;/li&gt;
&lt;li&gt;Stadium - Probably should have gone in the team table, seeing as I&amp;rsquo;m only dealing with league data.&lt;/li&gt;
&lt;li&gt;Game&lt;/li&gt;
&lt;li&gt;Appearance - A relationship table for players and games, due to the many to many relationship.&lt;/li&gt;
&lt;li&gt;Goal&lt;/li&gt;
&lt;li&gt;Assist - Could be stored in the goal table.&lt;/li&gt;
&lt;li&gt;Yellow Card&lt;/li&gt;
&lt;li&gt;Red Card - Yellow and red card could be stored in the same table.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;A final note - where you need an Autoincrementing primary key for your table, you don&amp;rsquo;t want to store that in your python object, nor do you want to store Foreign Key references in the object. Instead pass those as parameters to the Insert method.&lt;/p&gt;

&lt;h2 id=&#34;creating-the-sqlite-database-and-tables&#34;&gt;Creating the sqlite database and tables.&lt;/h2&gt;

&lt;p&gt;At the top of your script you want to connect to your SQLite database, if the database doesn&amp;rsquo;t exist, SQLite will make it for you. Man I love that feature. Then you want to create the tables, like so:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;import sqlite3 as lite

con = lite.connect(&#39;test.db&#39;)
cur = con.cursor()

cur.execute(&amp;quot;CREATE TABLE IF NOT EXISTS Season (id INTEGER PRIMARY KEY AUTOINCREMENT, Competition TEXT, Year INTEGER)&amp;quot;)

cur.execute(&amp;quot;CREATE TABLE IF NOT EXISTS Referee (Name TEXT PRIMARY KEY)&amp;quot;)

cur.execute(&amp;quot;CREATE TABLE IF NOT EXISTS Team (Name Text PRIMARY KEY)&amp;quot;)

cur.execute(&amp;quot;CREATE TABLE IF NOT EXISTS Player (Name TEXT PRIMARY KEY, DOB INTEGER, Nation TEXT, Goalkeeper INTEGER)&amp;quot;)

cur.execute(&amp;quot;CREATE TABLE IF NOT EXISTS Stadium (Name TEXT PRIMARY KEY, TeamName TEXT, FOREIGN KEY(TeamName) REFERENCES Team(Name))&amp;quot;)
# and so on and so on...
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Notice that I don&amp;rsquo;t use any kind of ORM. Personally, I like SQL, and I prefer to write it directly. I think it&amp;rsquo;s good skill to have in data science, you won&amp;rsquo;t always be able to use an ORM, but you can always query with SQL.&lt;/p&gt;

&lt;p&gt;Then create your classes. For example:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;class Competition:
    def __init__(self, league, year):
        self.league = league
        self.year = year

    def __str__(self):
        return &amp;quot;League: {}, Year:{}&amp;quot;.format(self.league, self.year)

    def insert(self, con, cur):
        cur.execute(&amp;quot;INSERT OR IGNORE INTO Season (Competition, Year) VALUES (?, ?)&amp;quot;, (self.league, self.year))
        con.commit()


class Referee:
    def __init__(self, name):
        self.name = name

    def __str__(self):
        return &amp;quot;Referee: {}&amp;quot;.format(self.name)

    def insert(self, con, cur):
        cur.execute(&amp;quot;INSERT OR IGNORE INTO Referee (Name) VALUES (?)&amp;quot;, (self.name,))
        con.commit()


class Team:
    def __init__(self, name):
        self.name = name

    def __str__(self):
        return &amp;quot;Team: {}&amp;quot;.format(self.name)

    def insert(self, con, cur):
        cur.execute(&amp;quot;INSERT OR IGNORE INTO Team (Name) VALUES (?)&amp;quot;, (self.name,))
        con.commit()


class Player:
    def __init__(self, name, DOB, nationality, goalkeeper):
        self.name = name
        self.DOB = DOB
        self.nationality = nationality
        #Goalkeeper is a boolean value
        self.goalkeeper = goalkeeper

    def __str__(self):
        return &amp;quot;PLAYER - Name: {}, DOB: {}, Nationality: {}, Is Goalkeeper: {}&amp;quot;.format(self.name, self.DOB, self.nationality, self.goalkeeper)

    def insert(self, con, cur):
        cur.execute(&amp;quot;INSERT OR IGNORE INTO Player (Name, DOB, Nation, Goalkeeper) VALUES (?, ?, ?, ?)&amp;quot;, (self.name, self.DOB, self.nationality, self.goalkeeper))
        con.commit()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The init and str methods are pretty standard fare. You&amp;rsquo;ll notice that on the insert methods, we pass the cursor and the connection to the method. It&amp;rsquo;s just a good way of being concise. It also make sure we don&amp;rsquo;t initialise a database connection every time we insert data. We do however commit after every insertion. I like to use judicious printing and regular committing, so I can pick up where I left off if something goes wrong.&lt;/p&gt;

&lt;p&gt;Other things to note, I use player names, team names and other such names as primary keys. You see, under the hood SQLite3 doesn&amp;rsquo;t actually make primary keys, it just makes unique indices. I know there were no two players called exactly the same thing so that also simplifies things. It also means we don&amp;rsquo;t have to insert and then retrieve the id when doing insertions. And we can do a simple &amp;ldquo;INSERT OR IGNORE&amp;rdquo; to prevent duplicates.&lt;/p&gt;

&lt;h2 id=&#34;looping-through&#34;&gt;Looping through&lt;/h2&gt;

&lt;p&gt;So now that you&amp;rsquo;ve created your tables, made your classes and given each class an insert method, you just need to loop through the JSON files and make all the insertions. So it starts like this&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;for index in range(380): #For 380 games in a premier league season.
    file = &#39;game{}.json&#39;.format(index)
    with open(file, &#39;r&#39;) as data_file:
        data = json.load(data_file)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Then the insertion might look something like this. Remember, JSON objects are mapped to dicts so it works just like accessing nested elements in a dict.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;players = []
for teamList in data[&amp;quot;teamLists&amp;quot;]:
    for player in teamList[&amp;quot;lineup&amp;quot;]:
        if player[&amp;quot;matchPosition&amp;quot;] == &amp;quot;G&amp;quot;:
            players.append(Player(player[&amp;quot;name&amp;quot;][&amp;quot;display&amp;quot;], player[&amp;quot;birth&amp;quot;][&amp;quot;date&amp;quot;][&amp;quot;millis&amp;quot;], player[&amp;quot;birth&amp;quot;][&amp;quot;country&amp;quot;][&amp;quot;country&amp;quot;], 1))
        else:
            players.append(Player(player[&amp;quot;name&amp;quot;][&amp;quot;display&amp;quot;], player[&amp;quot;birth&amp;quot;][&amp;quot;date&amp;quot;][&amp;quot;millis&amp;quot;], player[&amp;quot;birth&amp;quot;][&amp;quot;country&amp;quot;][&amp;quot;country&amp;quot;], 0))

for player in players:
    player.insert(con, cur)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Apologies if the code wrapping makes that tricky to read. So we make a whole bunch of player objects and put them in a list called players, then we loop through the players list and insert the data into the database. Simples.&lt;/p&gt;

&lt;h2 id=&#34;there-you-go&#34;&gt;There you go&lt;/h2&gt;

&lt;p&gt;So this wasn&amp;rsquo;t exactly a walkthrough, but I hope you understand a possible process to scrape JSON data from a website, map it to a domain using basic Domain Driven Design, and creating a normalised SQLite database from the data. The only thing left to do is to query the data and save the results as a csv file that you can then use in a D3.js animation.&lt;/p&gt;

&lt;h2 id=&#34;querying-the-sqlite-database&#34;&gt;Querying the sqlite database&lt;/h2&gt;

&lt;p&gt;So I recommend creating an SQL file. Call it something clever like my_query.sql. Then in the terminal, you want to test your query on stdout first. so you would type&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;sqlite3 mydatabase.db
&amp;gt;&amp;gt;&amp;gt; .mode column
&amp;gt;&amp;gt;&amp;gt; .headers ON
&amp;gt;&amp;gt;&amp;gt; .read my_query.sql
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;If you don&amp;rsquo;t like the results, tweak your query and run it again.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;&amp;gt;&amp;gt;&amp;gt; .read my_query.sql
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Now your happy with it. For example this to get a list of all players and how many goals they scored.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;SELECT player.name AS players, 
       COUNT(goal.id) AS goals 
  FROM player 
       LEFT JOIN goal 
       ON player.name = goal.player
 GROUP BY players
 ORDER BY goals DESC;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Then in the sqlite command you would do this.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;&amp;gt;&amp;gt;&amp;gt; .mode csv
&amp;gt;&amp;gt;&amp;gt; .output my_results.csv
&amp;gt;&amp;gt;&amp;gt; .read my_query.sql
&amp;gt;&amp;gt;&amp;gt; .exit
open my_results.csv
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;And there you have it, the easiest way to get csv data from SQlite. No python imports, no worrying about UTF-8, no csv writers etc. Just simple. Now, &lt;a href=&#34;https://davejkane.github.io/D3-CDF-with-hover&#34;&gt;creating a graph from the csv data.&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Webscraping for Fun and Stats</title>
      <link>https://davejkane.github.io/post/webscraping-fun-stats/</link>
      <pubDate>Sun, 20 Nov 2016 00:00:00 +0000</pubDate>
      
      <guid>https://davejkane.github.io/post/webscraping-fun-stats/</guid>
      <description>

&lt;h2 id=&#34;webscraping&#34;&gt;Webscraping&lt;/h2&gt;

&lt;p&gt;Webscraping is the art of programmatically browsing the web to retrieve information. It&amp;rsquo;s fun. You get to feel a bit like a hacker, even though you probably don&amp;rsquo;t actually want to hack into non-public facing websites to get data. And for a lot of publically available data, it is the most practical way to get it.&lt;/p&gt;

&lt;p&gt;Webscraping is not web crawling. Web crawling is more open ended and basically just used to index websites, rather than extract information. Webscraping can well be looked down upon as an evil act, with connotations of DDoS and cracking. But it&amp;rsquo;s also possible to scrape data from a website politely, without burdoning the webserver and breaching terms of use.&lt;/p&gt;

&lt;p&gt;The hardest bit about web scraping is findning good information and tools. And that&amp;rsquo;s where I hope this article can help. Many web scraping tools focus on interacting with the html returned from a HTTP request, but with the rise of modern javascript rendered websites, that approach is pretty limited, because increasingly the information you see on the page was sent with secondary requests, usually in the form of JSON or xml.&lt;/p&gt;

&lt;h2 id=&#34;the-solution-headless-web-browsing&#34;&gt;The solution - Headless Web Browsing.&lt;/h2&gt;

&lt;p&gt;The solution to this is to use a headless web browser and browse the site programmatically. A headless web browser is just a browser without a graphical interface, so you pretty much have to use programming to control it. But it&amp;rsquo;s easy. If you&amp;rsquo;re reading this you&amp;rsquo;ve already done the hard part of finding decent info. If I do say so myself.&lt;/p&gt;

&lt;h2 id=&#34;the-tools&#34;&gt;The tools&lt;/h2&gt;

&lt;p&gt;We&amp;rsquo;re going to use python for this. It&amp;rsquo;s the goto language for such scripty things, but if your favourite programming language supports Selenium Web Driver, and it probably does, then feel free to use that instead. I hope the information in the rest of this post will still be useful to you.&lt;/p&gt;

&lt;h3 id=&#34;selenium-web-driver&#34;&gt;Selenium Web Driver&lt;/h3&gt;

&lt;p&gt;First install Selenium web driver. Selenium web driver provides an interface for you to control a browser programmatically. Just open up your favourite search engine, probably Bing, and search for &amp;ldquo;install selenium webdriver [your favourite programming language] [your operating system]&amp;rdquo;. Follow those instructions. This post isn&amp;rsquo;t the place to list all of the combinations of architectures and programming languages.&lt;/p&gt;

&lt;h3 id=&#34;phantomjs&#34;&gt;PhantomJS&lt;/h3&gt;

&lt;p&gt;Next you&amp;rsquo;re going to have to install PhantomJS. PhantomJS is the actual headless web browser that you will be controlling with Selenium. You&amp;rsquo;ll need NodeJS and the NPM to install it, so if you don&amp;rsquo;t have Node installed, head over to &lt;a href=&#34;https://nodejs.org&#34;&gt;nodejs.org&lt;/a&gt; and follow the installaton instructions for your computer.&lt;/p&gt;

&lt;p&gt;Once you&amp;rsquo;ve done that you can install PhantomJS from npm by typing the following into a terminal.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;npm install phantomjs -g 
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;acessing-a-page&#34;&gt;Acessing a page&lt;/h2&gt;

&lt;p&gt;Create a new python script &lt;strong&gt;whatever_you_want_to_call_it.py&lt;/strong&gt; and open it up in your favourite text editor and enter the following:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;from selenium import webdriver
import time

driver = webdriver.PhantomJS()
driver.set_window_size(1024, 768)
driver.get(&#39;https://google.com&#39;)
print &#39;Sleeping&#39;
time.sleep(3)
driver.save_screenshot(&#39;screen.png&#39;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Go ahead and run your script with&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;python whatever_you_called_your_file.py
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Then&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;open screen.png
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Wow, huh. pretty, pretty, pretty, pretty good. Consider that the &amp;ldquo;hello world&amp;rdquo; of webscraping. If you got it to work then you&amp;rsquo;re ready to scrape the web for fun and stats. That little sleep in there, that&amp;rsquo;s just to make sure all the javascript has rendered on the page. Not really a big deal when you&amp;rsquo;re only hitting up google.&lt;/p&gt;

&lt;h2 id=&#34;inspecting-with-developer-tools&#34;&gt;Inspecting with developer tools.&lt;/h2&gt;

&lt;p&gt;Right, I&amp;rsquo;m hoping you came here to find out how to get sports stats or some other publically available statistics, after reading my post &lt;a href=&#34;https://davejkane.github.io/does-football-follow-pareto&#34;&gt;Does football follow the pareto principal?&lt;/a&gt; So the first thing your going to want to do is head over to &lt;a href=&#34;http://www.bbc.com/sport/football/results&#34;&gt;BBC football results&lt;/a&gt; with chrome. I hope you&amp;rsquo;re using chrome, if not, try to follow along anyway, but make sure you&amp;rsquo;re using a browser with some sort of decent developer tools. For the record, I did not get my stats from the BBC, but I can&amp;rsquo;t tell you where I got them. Even though sports statistics are public knowledge, scraping a website to steal, I mean scrape them, probably goes against some kind of terms of service and is most likely frowned upon.&lt;/p&gt;

&lt;p&gt;Anyhoo, find a game that has a report link on the right side. Right click on the link and Inspect Element. I think that&amp;rsquo;s what it&amp;rsquo;s called, my chrome is in Danish. That will take you to the location of an anchor element for that link. Now in the element inspector, right click on the anchor element and select copy &amp;gt; copy selector.&lt;/p&gt;

&lt;p&gt;Cool now we&amp;rsquo;re ready to get the link for that programmatically.&lt;/p&gt;

&lt;h2 id=&#34;retrieving-information-with-selenium&#34;&gt;Retrieving information with selenium&lt;/h2&gt;

&lt;p&gt;So let&amp;rsquo;s change our script to:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;from selenium import webdriver
import time

driver = webdriver.PhantomJS()
driver.set_window_size(1024, 768)
driver.get(&#39;https://google.com&#39;)
print &#39;Sleeping&#39;
time.sleep(3)
link = driver.find_element_by_css_selector(&#39;PASTE_YOUR_SELECTOR_HERE&#39;)
print link.get_attribute(&#39;href&#39;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Alternatively you could have copied the XPath from developer tools and used&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;driver.find_element_by_xpath()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;instead. I prefer css selectors just because I&amp;rsquo;m used to them from jquery.&lt;/p&gt;

&lt;p&gt;So there you go, you just programmatically got some information from a webpage. Pretty sweet.
Now, you&amp;rsquo;ll notice that the anchor tag there has a class of report, so what we could instead have done, is return a list of all the report anchors like so:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;links = driver.find_elements_by_css_selector(&#39;a.report&#39;)
for link in links:
    print link.get_attribute(&#39;href&#39;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Now we&amp;rsquo;re talking. Notice that was find_element&lt;strong&gt;s&lt;/strong&gt;_by_css_selector, that extra s in there gives us a list of all matches.&lt;/p&gt;

&lt;h2 id=&#34;interacting-with-the-page&#34;&gt;Interacting with the page&lt;/h2&gt;

&lt;p&gt;So sometimes, we&amp;rsquo;re going to have to interact with the page, such as clicking on things, or the bane of all webscrapers, scrolling down. Now a lot of tutorials online make scrolling down a web page sound way more complicated than it ought to be. It should in fact be this easy&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;def scroll():
   print &#39;Scrolling&#39;
   driver.execute_script(&amp;quot;window.scrollTo(0, document.body.scrollHeight);&amp;quot;)
   time.sleep(5)

for _ in range(12):
    scroll()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;That scrolls to the bottom of the page 12 times, waiting for 5 seconds each time to allow items to load. We&amp;rsquo;ve used driver.execute_script() to allow us to use javascript directly on the page. We can use that to click on elements as well. Say there was a tab called &amp;ldquo;stats&amp;rdquo; that didn&amp;rsquo;t have a href and it was the third on a tab bar. We can get the selector from developer tools and click on the tab using the following script.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;statsbutton = driver.find_element_by_css_selector(&#39;div.tabs &amp;gt; ul &amp;gt; li:nth-child(3)&#39;)
driver.execute_script(&amp;quot;arguments[0].click();&amp;quot;, statsbutton) 
print &#39;Clicking&#39;
time.sleep(5)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;again we add a sleep event just to allow any javascript to load. 5 seconds is probably a bit conservative, you can most likely go a bit shorter than that.&lt;/p&gt;

&lt;h2 id=&#34;looping-through-multiple-web-pages&#34;&gt;Looping through multiple web pages&lt;/h2&gt;

&lt;p&gt;So what if you scrape a page to get a whole bunch of urls and then you want to go to each of those pages and get some more information, for example go to each game of the season and collect stats. Well, my recommendation would be to first write your list of urls to file. Because scripting is sometimes a pain in the ass, and you might make a mistake. So you don&amp;rsquo;t want to have to go to the front page every time you make a typo and trawl for links that you&amp;rsquo;ve already collected. So, to write to a file, simply:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;file = &#39;gamelinks.txt&#39;
target = open(file, &#39;w&#39;)

for link in links: #links collected earlier in the script.
target.write(&amp;quot;%s\n&amp;quot; % link.get_attribute(&#39;href&#39;))
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Then you want to create a new script that goes through your text file and gets the stats. First get the links into a list.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;links = [link.rstrip(&#39;\n&#39;) for link in open(&#39;gamelinks.txt&#39;)]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Then you want to loop through the links. Here is the complete example script.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;from selenium import webdriver
from selenium.common.exceptions import TimeoutException
import time
import io

driver = webdriver.PhantomJS() # or add to your PATH
driver.set_window_size(1024, 768) # optional

links = [link.rstrip(&#39;\n&#39;) for link in open(&#39;fixtures.txt&#39;)]
linksLength = len(links)

for index, value in enumerate(links):
    print &#39;Starting&#39;
    driver.set_page_load_timeout(60)
    while True:
        try:
            driver.get(value)
        except TimeoutException:
            print &amp;quot;Timeout, retrying...&amp;quot;
            continue
        else:
            break
    print &#39;Sleeping&#39;
    time.sleep(3)
    dataContainer = driver.find_element_by_css_selector(&#39;div.dataContainer&#39;)
    dataJSON = dataContainer.get_attribute(&#39;data-game&#39;)
    file = &#39;data{}.json&#39;.format(index)
    target = io.open(file, &#39;w&#39;, encoding=&amp;quot;utf-8&amp;quot;)
    target.write(dataJSON)
    target.close()
    print &#39;File {} of {} written&#39;.format(index, linksLength - 1)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;So let&amp;rsquo;s go through that. First the usual imports, plus selenium&amp;rsquo;s TimeoutExeption. That&amp;rsquo;s because if a page is too slow, it will crash your script and you&amp;rsquo;ll have to do over, and we don&amp;rsquo;t want that. We&amp;rsquo;ll also import io for writing json, because footballers often have funny names that aren&amp;rsquo;t ascii, so we need utf-8.&lt;/p&gt;

&lt;p&gt;We make an enumerated for loop over the links so that we have access to the index for file naming and printing purposes. You&amp;rsquo;ll notice that there&amp;rsquo;s a lot of printing statements in there. I like to know what&amp;rsquo;s going on while my scripts are running. Web scraping scripts can be excruciatingly slow, so those print statements are a sanity check.&lt;/p&gt;

&lt;p&gt;We do a try except on the driver.get so that we can handle pages that are taking too long.&lt;/p&gt;

&lt;p&gt;So in this example we just happened to stumble upon a website that stores it&amp;rsquo;s data in json format right in a data attribute on the page. Yes this awesomeness does sometimes happen in the real world. In fact, that&amp;rsquo;s how I got the premier league stats from an unidentified website. The rest of the script saves each game&amp;rsquo;s json object to it&amp;rsquo;s own file. Again this is for ease of working with. Also, there are 380 games in a premier league season, if you&amp;rsquo;re script craps out after 250 games or something, you really don&amp;rsquo;t want to start from that start again, as this could take hours.&lt;/p&gt;

&lt;h2 id=&#34;awesome&#34;&gt;Awesome&lt;/h2&gt;

&lt;p&gt;So there you go, that&amp;rsquo;s how to scrape a whole bunch of javascipt rendered webpages that follow a similar template. Easy peasy. It&amp;rsquo;s slow, much slower that pinging the server for the html page, but that&amp;rsquo;s not going to get you much in the days of the single page app.&lt;/p&gt;

&lt;h2 id=&#34;what-now&#34;&gt;What now?&lt;/h2&gt;

&lt;p&gt;So now that we have a whole bunch of JSON files, what are we going to do with them? Well, I suggest storing your data in an SQLite database, and that&amp;rsquo;s what my next post is all about. So join me to see &lt;a href=&#34;https://davejkane.github.io/json-sqlite-csv&#34;&gt;how I made an SQLite file&lt;/a&gt; that contains all of the stats from the &lt;sup&gt;2015&lt;/sup&gt;&amp;frasl;&lt;sub&gt;16&lt;/sub&gt; English Premier League season.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>